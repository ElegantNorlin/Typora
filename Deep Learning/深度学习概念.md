### 知识蒸馏

**原文链接：**[](https://zhuanlan.zhihu.com/p/92166184)

**原文链接：**[](https://zhuanlan.zhihu.com/p/92269636?from_voters_page=true)

在监督学习里也是这样的，在训练模型时，我们通常采用复杂模型或者Ensemble方式来获取最好的结果，导致参数冗余严重，像BERT里有3亿参数。因此在前向预测时，需要对模型进行复杂的计算（或多个模型加权），导致工程性能较差。

Hinton在NIPS 2014workshop中提出知识蒸馏（Knowledge Distillation，下面简称KD）概念：

> 把复杂模型或者多个模型Ensemble（Teacher）学到的知识 迁移到另一个轻量级模型（ Student ）上叫**知识蒸馏。使**模型变轻量的同时（方便部署），尽量不损失性能。

从定义上来看KD属于模型压缩、加速的一类玩法。（后面的研究也会将KD应用于模型表现的提升）。

在这里，知识应该宽泛和抽象地理解，模型参数，网络层的输出（网络提取的特征）、网络输出等都可以理解为知识。



**Distilling the Knowledge in a Neural Network 》** **【Meta info】**Hinton，NIPS 2014 workshop，Cites：2400

这篇paper是知识蒸馏的开山之作，由Hinton老爷子在NIPS 2014 workshop上提出，文章的思路非常简单、优雅。

首先，我们对一些术语进行定义：

- Teacher：原始较大的模型或模型Ensemble，用于获取知识
- Student：新的较小的模型，接收teacher的知识，训练后用于前向预测
- Hard target：样本原本的标签，One-hot
- Soft target ：Teacher输出的预测结果（一般是softmax之后的概率）

![img](https://gitee.com/wanwanzh/imagebed/raw/master/pictures/v2-c658d81ce218dc59e74ebe167ad0bb6b_1440w.jpg) 

![img](https://gitee.com/wanwanzh/imagebed/raw/master/pictures/v2-b530c7c3baa20356af5523ba3cb6444d_1440w.jpg) 





### 泛化能力

**泛化能力的定义**

概括地说，泛化能力（generalization ability）是指[机器学习算法](https://baike.baidu.com/item/机器学习算法/18635836)对新鲜样本的适应能力，简而言之是在原有的数据集上添加新的数据集，通过训练输出一个合理的结果。学习的目的是学到隐含在数据背后的规律，对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该能力称为泛化能力。

**泛化能力的性质**

 通常期望经训练样本训练的网络具有较强的泛化能力，也就是对新输入给出合理响应的能力。应当指出并非训练的次数越多越能得到正确的输入输出映射关系。网络的性能主要用它的泛化能力来衡量。







### MLP

**MLP** （Multi-Layer Perceptron）是一种前向结构的人工神经网络，映射一组输入向量到一组输出向量。MLP可以被看做是一个有向图，由多个节点层组成，每一层全连接到下一层。除了输入节点，每个节点都是一个带有非线性激活函数的神经元（或称处理单元）。







### Ensemble（集成学习）

集成学习也称为模型融合(Model Ensemble)。是一种有效提升机器学习效果的方法。
不同于传统的机器学习方法在训练集上构建一个模型，集成学习通过构建并融合多个模型来完成学习任务。

可能概念不好理解，我们通过下图的一个例子来介绍集成的基本概念，下图是一个二分类问题。

![在这里插入图片描述](https://gitee.com/wanwanzh/imagebed/raw/master/pictures/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JvY2t5NjY4OA==,size_16,color_FFFFFF,t_70.png) 

据均是测试数据，a、b、c分别是三个不同模型在测试集上的分类结果。需要注意的是，一般在做集成的时候，相同模型结构的不同参数是比较常见的情况(同质模型融合(Homogeneous Model Ensemble))。但是不能死板，我们也可以把不同模型甚至完全不同的算法做集成学习(异质模型融合(Heterogeneous Model Ensemble))。

上图中每个模型的错误分类的样本都用圆圈标了出来，可以看到每个模型大体都是准确的，但是错误的情况也会存在。这种情况下，我们可以考虑用集成的方法:将3个不同模型的记过进行综合考虑，因为错误分类毕竟是少数，所以最终的结果的可靠性就会增加。

上图d模型就是进行集成学习的结果，可以看到效果变好了。这里采用的方法是多数投票(majority vote):对3个模型分类结果中多数的结果作为最终结果。除了多数投票，在实际应用中常见的方法还有取均值或者中值，但是基本思想都是一样的。“三个臭皮匠，赛过诸葛亮”。

以上，**集成学习主要包含两个步骤:**

* 训练若干个单模型(Signle Model)，也称作基学习器(Base Learner)。这些单模型可以是决策树、神经网络或者是其他类型的算法。
* 模型融合，相应的方法有平均法、投票法和学习法等等。我们不单单可以进行一级模型融合，我们也可以进行多级模型融合，或者是一级模型和二级模型的混合融合类。

集成学习的泛化能力要比单模型强得多，这也是集成学习能够被人们广泛研究和应用的原因。集成学习可以整合多个"弱"模型最终得到一个"强"模型。

集成法是通过多个模型的结果综合考虑给出最终的结果，虽然准确率和稳定性都会提高，但是训练多个模型的计算成本也是非常高的，如果训练10个左右的模型，则计算成本高了一个量级。





### ground truth

Ground-truth在机器学习中表示有监督学习的训练集的分类准确性。

Ground-truth	就是人工标注的结果。比如目标检测中，模型预测的框是要和ground truth（也就是人工标注的框）做比较的。







### ROI Align

对获得的ROI区域执行论文提出的ROIAlign操作，即先将原图和feature map的pixel对应起来，然后将feature map和固定的feature对应起来。

为了缓解ROI Pooling量化误差过大的缺点，ROIAligin没有使用量化操作，而是使用了双线性插值。它充分的利用原图中的虚拟像素值如四周的四个真实存在的像素值来共同决定目标图中的一个像素值，即可以将和类似的非整数坐标值像素对应的输出像素值估计出来。这一过程如下图所示：

![img](https://gitee.com/wanwanzh/imagebed/raw/master/pictures/webp-20210307105831095) 

其中featmap就是VGG16或者其他Backbone网络获得的特征图，黑色实线表示的是ROI划分方式，最后输出的特征图大小为，然后就使用双线性插值的方式来估计这些蓝色点的像素值，最后得到输出，然后再在橘红色的区域中执行Pooling操作最后得到的输出特征图。可以看到，这个过程相比于ROI Pooling没有引入任何量化操作，即原图中的像素和特征图中的像素是完全对齐的，没有偏差，这不仅会提高检测的精度，同时也会有利于实例分割。



